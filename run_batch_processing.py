import os
import glob
import torch
import trimesh
import pyrender
import numpy as np
import shutil
import csv
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from hy3dgen.texgen import Hunyuan3DPaintPipeline
from hy3dgen.text2image import HunyuanDiTPipeline
from evaluate_clip_ds import ModelManager as ClipModelManager, evaluate_clip_similarity



SOURCE_DATA_DIR = '/source/sola/dataset/3D-FUTURE-model-part1'
OUTPUT_DIR = 'outputs_batch'
NUM_OBJECTS_TO_PROCESS = 30 # ëª‡ê°œì˜ ê°ì²´? 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

START_INDEX = 1  # ì‹œì‘í•  ê°ì²´ ë²ˆí˜¸
END_INDEX = 60    # ì¢…ë£Œí•  ê°ì²´ ë²ˆí˜¸ (ì´ ë²ˆí˜¸ê¹Œì§€ í¬í•¨)

class ModelManager:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ëª¨ë¸ë“¤ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.models = {}
        print(f"Using device: {DEVICE}")

    def get_blip_model(self):
        if 'blip' not in self.models:
            print("Loading Image Captioning model (BLIP)...")
            processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
            model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(DEVICE)
            self.models['blip'] = (model, processor)
        return self.models['blip']

    def get_txt2img_pipe(self):
        if 'txt2img' not in self.models:
            print("Loading Txt2Img pipeline (HunyuanDiT)...")
            # HunyuanDiTPipelineì€ ë‚´ë¶€ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ë¯€ë¡œ í´ë˜ìŠ¤ ìì²´ë¥¼ ì €ì¥
            pipe = HunyuanDiTPipeline()
            self.models['txt2img'] = pipe
        return self.models['txt2img']

    def get_hunyuan_pipe(self):
        if 'hunyuan' not in self.models:
            print("Loading Hunyuan3D texturing pipeline...")
            pipe = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')
            self.models['hunyuan'] = pipe
        return self.models['hunyuan']





def generate_caption(model_manager, image_path):
    model, processor = model_manager.get_blip_model()
    raw_image = Image.open(image_path).convert("RGB")
    inputs = processor(raw_image, return_tensors="pt").to(DEVICE)
    out = model.generate(**inputs, max_new_tokens=30)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

def extract_object_type(caption):
    common_types = ['sofa', 'bed', 'chair', 'table', 'cabinet', 'shelf', 'desk', 'couch']
    caption_lower = caption.lower()
    for obj_type in common_types:
        if obj_type in caption_lower:
            return obj_type
    return "object"


def generate_simple_image(model_manager, prompt, output_path):
    """ë‹¨ìˆœ Txt2Imgë¡œ ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    pipe = model_manager.get_txt2img_pipe()
    generator = torch.manual_seed(42)
    # HunyuanDiTPipelineì€ seedë¥¼ ì§ì ‘ ë°›ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, torch.manual_seed ì‚¬ìš©
    result_image = pipe(prompt, seed=42) # íŒŒì´í”„ë¼ì¸ì— ë§ê²Œ í˜¸ì¶œ
    result_image.save(output_path)
    return output_path


def texture_mesh(model_manager, mesh_path, ref_image_path, output_path):
    pipe = model_manager.get_hunyuan_pipe()
    input_mesh = trimesh.load_mesh(mesh_path, force='mesh')
    ref_image = Image.open(ref_image_path).convert("RGBA")
    textured_mesh = pipe(mesh=input_mesh, image=ref_image)
    textured_mesh.export(output_path)
    return output_path


def save_source_with_texture(obj_dir, output_obj_path):
    """
    ë©”ì‹œë¥¼ ë¡œë“œí•˜ê³ , ì›ë³¸ í…ìŠ¤ì²˜ë¥¼ 'ìˆ˜ë™ìœ¼ë¡œ' ëª…ì‹œì ìœ¼ë¡œ ì ìš©í•œ í›„ ì €ì¥í•©ë‹ˆë‹¤.
    """
    source_obj_path = os.path.join(obj_dir, 'normalized_model.obj')
    source_texture_path = os.path.join(obj_dir, 'texture.png')
    
    # 1. ì›ë³¸ í…ìŠ¤ì²˜ íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
    if not os.path.exists(source_texture_path):
        print(f"âš ï¸ Source texture not found at {source_texture_path}. Skipping texture application.")
        # í…ìŠ¤ì²˜ê°€ ì—†ëŠ” ê²½ìš°, ì›ë³¸ ë©”ì‹œë§Œ ë³µì‚¬í•˜ê±°ë‚˜ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
        shutil.copy(source_obj_path, output_obj_path)
        return

    print(f"Applying source texture to mesh: {os.path.basename(source_obj_path)}")

    # 2. ì¬ì§ˆ/í…ìŠ¤ì²˜ ì •ë³´ ì—†ì´ ìˆœìˆ˜í•˜ê²Œ ì§€ì˜¤ë©”íŠ¸ë¦¬ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    #    'process=False'ëŠ” ë¶ˆí•„ìš”í•œ ìë™ ì²˜ë¦¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    mesh = trimesh.load(source_obj_path, force='mesh', process=False)

    # 3. ì›ë³¸ í…ìŠ¤ì²˜ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤.
    texture_image = Image.open(source_texture_path)
    
    # 4. ë¡œë“œí•œ í…ìŠ¤ì²˜ë¡œ ìƒˆë¡œìš´ ì¬ì§ˆ(material)ì„ ë§Œë“­ë‹ˆë‹¤.
    material = trimesh.visual.texture.SimpleMaterial(image=texture_image)
    
    # 5. ë©”ì‹œì˜ ì‹œê°ì  ì†ì„±ì— UV ì¢Œí‘œì™€ ìœ„ì—ì„œ ë§Œë“  ì¬ì§ˆì„ ëª…ì‹œì ìœ¼ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.
    #    ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ë¥¸ í…ìŠ¤ì²˜ê°€ ìºì‹œë˜ì–´ ìˆì–´ë„ ë¬´ì‹œí•˜ê³  ì´ í…ìŠ¤ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    mesh.visual = trimesh.visual.TextureVisuals(uv=mesh.visual.uv, material=material)
    
    # 6. í…ìŠ¤ì²˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì ìš©ëœ ë©”ì‹œë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
    mesh.export(output_obj_path)
    print(f"âœ… Saved textured source mesh to {output_obj_path}")


def main():
    print("Starting optimized batch processing job...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    models = ModelManager()
    clip_models = ClipModelManager()

    print("\nPhase 1: Finding all valid objects to select from...")
    all_source_dirs = sorted([d for d in glob.glob(f'{SOURCE_DATA_DIR}/*') if os.path.isdir(d)])
    
    all_valid_jobs = []
    for obj_dir in all_source_dirs:
        source_img_path = os.path.join(obj_dir, 'image.jpg')
        if not os.path.exists(source_img_path) or not os.path.exists(os.path.join(obj_dir, 'normalized_model.obj')):
            continue

        original_text = generate_caption(models, source_img_path)
        object_type = extract_object_type(original_text)

        if object_type != "object":
            job_info = {'dir': obj_dir, 'type': object_type, 'caption': original_text}
            all_valid_jobs.append(job_info)

    print(f"Found {len(all_valid_jobs)} total valid objects.")

    # ì§€ì •ëœ ë²”ìœ„ì˜ ê°ì²´ë§Œ ì„ íƒ
    if START_INDEX > len(all_valid_jobs):
        print(f"Start index ({START_INDEX}) is greater than the number of valid objects ({len(all_valid_jobs)}). No new objects to process.")
        return
    
    # 1-based ì¸ë±ìŠ¤ë¥¼ 0-based ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ë³€í™˜
    processing_jobs = all_valid_jobs[START_INDEX - 1:END_INDEX]
    print(f"--> Processing {len(processing_jobs)} objects from index {START_INDEX} to {END_INDEX}.\n")

    if not processing_jobs:
        print("No objects selected in the specified range. Exiting.")
        return

    # ì„ íƒëœ jobë“¤ë¡œë¶€í„° unique_object_types ìƒì„±
    unique_object_types = set(job['type'] for job in processing_jobs)

    # 1b: ìœ ë‹ˆí¬í•œ ê°ì²´ íƒ€ì…ê³¼ í…Œë§ˆì— ëŒ€í•œ ì°¸ì¡° ì´ë¯¸ì§€ ë¯¸ë¦¬ ìƒì„±
    print("\nPhase 2: Pre-generating all required reference images...")
    ref_image_dir = os.path.join(OUTPUT_DIR, "_reference_images")
    os.makedirs(ref_image_dir, exist_ok=True)
    
    themes = {
        # 1. ìŠ¤íŒ€í‘í¬ (Steampunk)
        "steampunk": "a masterpiece steampunk {}, intricate brass and copper gears, polished mahogany, detailed mechanical parts, cinematic lighting",
        # 2. ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ì•„ë¥´ë°ì½” (Art Deco)
        "art_deco": "a luxurious art deco style {}, carved from white marble with intricate gold inlay, elegant and geometric, studio lighting, high detail",
        # 3. ì§€ë¸Œë¦¬ ì• ë‹ˆë©”ì´ì…˜ (Ghibli Anime)
        "ghibli": "a {} in a cozy, beautiful anime style, hand-drawn with soft pastel colors, watercolor art, by Studio Ghibli, trending on pixiv",
        # 4. í´ë ˆì´ ì• ë‹ˆë©”ì´ì…˜ (Claymation)
        "claymation": "a charming claymation {}, stop-motion style, made of plasticine clay with visible fingerprints, Aardman Animations aesthetic, soft volumetric lighting",
        # 5. íŒíƒ€ì§€ ìƒë¬¼ ë°œê´‘ (Bioluminescent)
        "bioluminescent": "a magical {} made of bioluminescent plants and glowing mushrooms, intertwined with fantasy vines, ethereal, cinematic, Avatar movie style",
        "yellow": "a yellow {}",
        "cyberpunk": "a cyberpunk {}, made of dark chrome and glowing purple circuits",
        "wooden": "a cozy {}, made of warm, natural oak wood with a smooth finish",
        "glass": "a sleek, modern {}, made of translucent frosted glass with minimalist design",
        "medieval": "an ornate medieval-style {}, crafted from dark heavy wood and wrought iron details"
    }
    
    ref_image_paths = {}
    for obj_type in unique_object_types:
        ref_image_paths[obj_type] = {}
        for theme_name, prompt_template in themes.items():
            prompt = prompt_template.format(obj_type)
            output_path = os.path.join(ref_image_dir, f"ref_{obj_type}_{theme_name}.png")
            if not os.path.exists(output_path):
                print(f"Generating -> {output_path}")
                generate_simple_image(models, prompt, output_path)
            else:
                print(f"Skipping -> {output_path} (already exists)")
            ref_image_paths[obj_type][theme_name] = output_path
            
    # --- 2ë‹¨ê³„: ë©”ì¸ ì²˜ë¦¬ ---
    print("\nPhase 3: Processing all 3D objects using pre-generated images...")
    results_data = []
    object_counters = {}

    for i, job in enumerate(processing_jobs):
        obj_dir = job['dir']
        object_type = job['type']
        original_text = job['caption']
        
        print(f"\n--- [{i+1}/{NUM_OBJECTS_TO_PROCESS}] Processing Object: {os.path.basename(obj_dir)} ---")
        
        source_obj_path = os.path.join(obj_dir, 'normalized_model.obj')
        source_img_path = os.path.join(obj_dir, 'image.jpg')
        
        try:
            # 1. ê°ì²´ë³„ ì¹´ìš´í„°ë¥¼ themes ë£¨í”„ *ì‹œì‘ ì „*ì— í•œ ë²ˆë§Œ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
            object_counters[object_type] = object_counters.get(object_type, 0) + 1
            current_object_number = object_counters[object_type] + 100

            for theme_name, edited_prompt_template in themes.items():
                edited_prompt = edited_prompt_template.format(object_type)

                # 2. ìœ„ì—ì„œ ê³„ì‚°í•œ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë” ì´ë¦„ì„ ìƒì„±í•©ë‹ˆë‹¤.
                current_obj_name = f"{object_type}_{current_object_number}_{theme_name}"
                
                
                result_dir = os.path.join(OUTPUT_DIR, current_obj_name)
                os.makedirs(result_dir, exist_ok=True)
                print(f"\n-- Processing theme '{theme_name}' for {current_obj_name} --")

                pre_generated_ref_path = ref_image_paths[object_type][theme_name]
                ref_image_path_in_output = os.path.join(result_dir, 'ref_texture_image.png')
                shutil.copy(pre_generated_ref_path, ref_image_path_in_output)
                
                edited_mesh_path = texture_mesh(models, source_obj_path, ref_image_path_in_output, os.path.join(result_dir, 'edited_mesh.glb'))
                
                print(f"Evaluating similarity for {current_obj_name}...")

                save_source_with_texture(obj_dir, os.path.join(result_dir, 'source_mesh.obj'))
                source_obj_path_wtexture = os.path.join(result_dir, 'source_mesh.obj')
                score = evaluate_clip_similarity(
                    clip_models, 
                    source_obj_path_wtexture, 
                    edited_mesh_path, 
                    original_text, 
                    edited_prompt
                )
                
                
                shutil.copy(source_img_path, os.path.join(result_dir, 'source_image.jpg'))
                with open(os.path.join(result_dir, 'source_caption.txt'), 'w') as f: f.write(original_text)
                with open(os.path.join(result_dir, 'editing_prompt.txt'), 'w') as f: f.write(edited_prompt)
                
                results_data.append({
                    'object_type': current_obj_name,
                    'editing_prompt': edited_prompt,
                    'clip_ds_score': f"{score:.4f}"
                })
                print(f"âœ… Finished processing {current_obj_name}. Score: {score:.4f}")

        except Exception as e:
            print(f"âŒâŒâŒ FAILED to process {obj_dir}. Error: {e}")
            import traceback
            traceback.print_exc()

    # --- ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ ---
    csv_path = os.path.join(OUTPUT_DIR, 'results.csv')
    print(f"\nSaving final results to {csv_path}")
    with open(csv_path, 'w', newline='') as csvfile:
        fieldnames = ['object_type', 'editing_prompt', 'clip_ds_score']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results_data)

    print("\nğŸ‰ğŸ‰ğŸ‰ Batch processing complete! ğŸ‰ğŸ‰ğŸ‰")


if __name__ == '__main__':
    # xvfb-runì„ ì‚¬ìš©í•˜ì—¬ headless í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ë¨
    # xvfb-run --auto-servernum python run_batch_processing.py
    main()