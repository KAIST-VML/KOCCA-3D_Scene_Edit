import os
import glob
import torch
import trimesh
import pyrender
import numpy as np
import csv
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm # ì§„í–‰ ìƒí™©ì„ ë³´ê¸° ìœ„í•´ tqdm ì¶”ê°€

# --- ì„¤ì • ---
EVALUATION_DIR = 'outputs_batch' # í‰ê°€í•  ê²°ê³¼ë¬¼ë“¤ì´ ìˆëŠ” í´ë”
CSV_OUTPUT_PATH = 'evaluation_results.csv' # ìµœì¢… ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ì´ë¦„
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class ModelManager:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ëª¨ë¸ë“¤ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.models = {}
        print(f"Using device: {DEVICE}")

    def get_clip_model(self):
        if 'clip' not in self.models:
            print("Loading CLIP model for evaluation...")
            model_id = "openai/clip-vit-large-patch14"
            model = CLIPModel.from_pretrained(model_id).to(DEVICE)
            processor = CLIPProcessor.from_pretrained(model_id)
            self.models['clip'] = (model, processor)
        return self.models['clip']

def evaluate_clip_similarity(model_manager, original_mesh_path, edited_mesh_path, original_text, edited_text):
    """ë‹¨ì¼ ì •ë©´ ë·°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ CLIP Directional Similarityë¥¼ ê³„ì‚°"""
    clip_model, clip_processor = model_manager.get_clip_model()
    
    def get_embedding(text=None, image=None):
        if text:
            inputs = clip_processor(text=text, return_tensors="pt").to(DEVICE)
            return clip_model.get_text_features(**inputs)
        elif image:
            inputs = clip_processor(images=image, return_tensors="pt").to(DEVICE)
            return clip_model.get_image_features(**inputs)
        return None

    def render_front_view(mesh_path):
        mesh = trimesh.load_mesh(mesh_path, force='mesh')
        scene = trimesh.Scene(mesh)
        camera_transform = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,3],[0,0,0,1]])
        scene.camera_transform = camera_transform
        data = scene.save_image(resolution=(224, 224))
        return Image.open(trimesh.util.wrap_as_stream(data))

    original_render_img = render_front_view(original_mesh_path)
    edited_render_img = render_front_view(edited_mesh_path)
    
    with torch.no_grad():
        t_clip = get_embedding(text=original_text)
        t_hat_clip = get_embedding(text=edited_text)
        x_clip = get_embedding(image=original_render_img)
        x_hat_clip = get_embedding(image=edited_render_img)

        delta_text = t_clip - t_hat_clip
        delta_image = x_clip - x_hat_clip
        delta_text_norm = delta_text / torch.linalg.norm(delta_text)
        delta_image_norm = delta_image / torch.linalg.norm(delta_image)
        similarity = torch.dot(delta_image_norm.squeeze(), delta_text_norm.squeeze())
        
    return similarity.item()

def main():
    """
    ì§€ì •ëœ í´ë”ì˜ ëª¨ë“  ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ê³  CSVë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    print(f"ğŸš€ Starting evaluation of all objects in '{EVALUATION_DIR}'...")
    models = ModelManager()
    
    # í‰ê°€í•  ëª¨ë“  í•˜ìœ„ í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜´ (_reference_images ê°™ì€ í´ë”ëŠ” ì œì™¸)
    all_result_dirs = [d for d in glob.glob(os.path.join(EVALUATION_DIR, '*')) if os.path.isdir(d) and not os.path.basename(d).startswith('_')]
    
    if not all_result_dirs:
        print(f"No result directories found in '{EVALUATION_DIR}'. Exiting.")
        return

    print(f"Found {len(all_result_dirs)} result directories to evaluate.")
    
    evaluation_results = []

    # tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ
    for result_dir in tqdm(all_result_dirs, desc="Evaluating Objects"):
        object_name = os.path.basename(result_dir)
        
        try:
            # í‰ê°€ì— í•„ìš”í•œ íŒŒì¼ ê²½ë¡œ ì •ì˜
            edited_mesh_path = os.path.join(result_dir, 'edited_mesh.glb')
            original_mesh_path = os.path.join(result_dir, 'source_mesh.obj')
            source_caption_path = os.path.join(result_dir, 'source_caption.txt')
            editing_prompt_path = os.path.join(result_dir, 'editing_prompt.txt')

            # í•„ìš”í•œ íŒŒì¼ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
            required_files = [edited_mesh_path, original_mesh_path, source_caption_path, editing_prompt_path]
            if not all(os.path.exists(p) for p in required_files):
                print(f"\nâš ï¸ Skipping {object_name}: Missing one or more required files.")
                continue

            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            with open(source_caption_path, 'r') as f:
                original_text = f.read().strip()
            with open(editing_prompt_path, 'r') as f:
                edited_text = f.read().strip()
            
            # ì ìˆ˜ ê³„ì‚°
            score = evaluate_clip_similarity(models, original_mesh_path, edited_mesh_path, original_text, edited_text)
            
            # ê²°ê³¼ ì €ì¥
            evaluation_results.append({
                'object_name': object_name,
                'clip_ds_score': f"{score:.4f}",
                'editing_prompt': edited_text
            })

        except Exception as e:
            print(f"\nâŒ FAILED to process {object_name}. Error: {e}")

    # --- ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ ---
    if not evaluation_results:
        print("No objects were successfully evaluated.")
        return

    print(f"\nSaving {len(evaluation_results)} evaluation results to {CSV_OUTPUT_PATH}...")
    
    with open(CSV_OUTPUT_PATH, 'w', newline='') as csvfile:
        fieldnames = ['object_name', 'clip_ds_score', 'editing_prompt']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(evaluation_results)

    print("\nğŸ‰ğŸ‰ğŸ‰ Evaluation complete! ğŸ‰ğŸ‰ğŸ‰")

if __name__ == '__main__':
    # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë Œë”ë§ì„ ìœ„í•´ headless í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: xvfb-run --auto-servernum python evaluate_clip_ds.py
    main()