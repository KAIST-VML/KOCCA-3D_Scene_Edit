import os
import glob
import torch
import trimesh
import pyrender
import numpy as np
import shutil
import csv
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from hy3dgen.texgen import Hunyuan3DPaintPipeline


SOURCE_DATA_DIR = '/source/sola/dataset/3D-FUTURE-model-part1'
OUTPUT_DIR = 'outputs_batch'
NUM_OBJECTS_TO_PROCESS = 25
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


class ModelManager:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ëª¨ë¸ë“¤ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.models = {}
        print(f"Using device: {DEVICE}")

    def get_blip_model(self):
        if 'blip' not in self.models:
            print("Loading Image Captioning model (BLIP)...")
            processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
            model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(DEVICE)
            self.models['blip'] = (model, processor)
        return self.models['blip']

    def get_controlnet_pipe(self):
        if 'controlnet' not in self.models:
            print("Loading ControlNet-SDXL pipeline...")
            controlnet = ControlNetModel.from_pretrained(
                "diffusers/controlnet-depth-sdxl-1.0", torch_dtype=torch.float16
            )
            pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                controlnet=controlnet, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
            ).to(DEVICE)
            self.models['controlnet'] = pipe
        return self.models['controlnet']
        
    def get_hunyuan_pipe(self):
        if 'hunyuan' not in self.models:
            print("Loading Hunyuan3D texturing pipeline...")
            pipe = Hunyuan3DPaintPipeline.from_pretrained('tencent/Hunyuan3D-2')
            self.models['hunyuan'] = pipe
        return self.models['hunyuan']

    def get_clip_model(self):
        if 'clip' not in self.models:
            print("Loading CLIP model for evaluation...")
            model_id = "openai/clip-vit-large-patch14"
            model = CLIPModel.from_pretrained(model_id).to(DEVICE)
            processor = CLIPProcessor.from_pretrained(model_id)
            self.models['clip'] = (model, processor)
        return self.models['clip']




def generate_caption(model_manager, image_path):
    model, processor = model_manager.get_blip_model()
    raw_image = Image.open(image_path).convert("RGB")
    inputs = processor(raw_image, return_tensors="pt").to(DEVICE)
    out = model.generate(**inputs, max_new_tokens=30)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

def extract_object_type(caption):
    common_types = ['sofa', 'bed', 'chair', 'table', 'cabinet', 'shelf', 'desk', 'couch']
    caption_lower = caption.lower()
    for obj_type in common_types:
        if obj_type in caption_lower:
            return obj_type
    return "object"


def create_depth_map(mesh_path, output_path):
    tri_mesh = trimesh.load_mesh(mesh_path, force="mesh")
    mesh = pyrender.Mesh.from_trimesh(tri_mesh, smooth =False)
    scene = pyrender.Scene(ambient_light=[0.1, 0.1, 0.3], bg_color=[0,0,0,0])
    scene.add(mesh, 'mesh')
    camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)
    camera_pose = np.array([[1,0,0,0], [0,1,0,0], [0,0,1,3], [0,0,0,1]])
    scene.add(camera, pose = camera_pose)
    renderer = pyrender.OffscreenRenderer(viewport_width=512, viewport_height=512)
    depth = renderer.render(scene, flags = pyrender.RenderFlags.DEPTH_ONLY)
    renderer.delete()
    if depth.max() > 0:
        depth_normalized = (depth / depth.max() * 255).astype(np.uint8)
    else:
        depth_normalized = np.zeros_like(depth, dtype = np.uint8)
    depth_image = Image.fromarray(depth_normalized)
    depth_image.save(output_path)
    return output_path

def create_ref_image(model_manager, control_image_path, prompt, output_path):
    pipe = model_manager.get_controlnet_pipe()
    control_image = Image.open(control_image_path).convert("RGB")
    generator = torch.manual_seed(42)
    result_image = pipe(
        prompt, image=control_image, num_inference_steps=30, generator=generator, controlnet_conditioning_scale=0.7
    ).images[0]
    result_image.save(output_path)
    return output_path

def texture_mesh(model_manager, mesh_path, ref_image_path, output_path):
    pipe = model_manager.get_hunyuan_pipe()
    input_mesh = trimesh.load_mesh(mesh_path, force='mesh')
    ref_image = Image.open(ref_image_path).convert("RGBA")
    textured_mesh = pipe(mesh=input_mesh, image=ref_image)
    textured_mesh.export(output_path)
    return output_path

def render_front_view(mesh_path):
    """ë©”ì‹œë¥¼ ê³ ì •ëœ ì •ë©´ ë·° í•˜ë‚˜ë¡œ ë Œë”ë§í•˜ì—¬ PIL Image ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    mesh = trimesh.load_mesh(mesh_path, force='mesh')
    scene = trimesh.Scene(mesh)
    # ì •ë©´ ë·°ë¥¼ ìœ„í•œ ê³ ì •ëœ ì¹´ë©”ë¼ ìœ„ì¹˜
    camera_transform = np.array([
        [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 3], [0, 0, 0, 1]
    ])
    scene.camera_transform = camera_transform
    # CLIP ëª¨ë¸ì˜ í‘œì¤€ ì…ë ¥ í•´ìƒë„ì¸ 224x224ë¡œ ë Œë”ë§
    data = scene.save_image(resolution=(224, 224))
    return Image.open(trimesh.util.wrap_as_stream(data))

def evaluate_clip_similarity(model_manager, original_mesh_path, edited_mesh_path, original_text, edited_text):
    """ë‹¨ì¼ ì •ë©´ ë·°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ CLIP Directional Similarityë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    clip_model, clip_processor = model_manager.get_clip_model()
    
    def get_embedding(text=None, image=None):
        # ì´ ë‚´ë¶€ í•¨ìˆ˜ëŠ” ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        if text:
            inputs = clip_processor(text=text, return_tensors="pt").to(DEVICE)
            return clip_model.get_text_features(**inputs)
        elif image:
            inputs = clip_processor(images=image, return_tensors="pt").to(DEVICE)
            return clip_model.get_image_features(**inputs)
        return None

    # 1. ì›ë³¸ê³¼ í¸ì§‘ëœ ë©”ì‹œë¥¼ ê°ê° ì •ë©´ ë·°ë¡œ ë Œë”ë§
    original_render_img = render_front_view(original_mesh_path)
    edited_render_img = render_front_view(edited_mesh_path)
    
    with torch.no_grad():
        # 2. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ
        t_clip = get_embedding(text=original_text)
        t_hat_clip = get_embedding(text=edited_text)
        x_clip = get_embedding(image=original_render_img)
        x_hat_clip = get_embedding(image=edited_render_img)

        # 3. ë°©í–¥ì„± ìœ ì‚¬ë„ ê³„ì‚°
        delta_text = t_clip - t_hat_clip
        delta_image = x_clip - x_hat_clip
        delta_text_norm = delta_text / torch.linalg.norm(delta_text)
        delta_image_norm = delta_image / torch.linalg.norm(delta_image)
        similarity = torch.dot(delta_image_norm.squeeze(), delta_text_norm.squeeze())
        
    return similarity.item()

def main():
    print("Starting batch processing job...")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    models = ModelManager()
    
    source_dirs = sorted([d for d in glob.glob(f'{SOURCE_DATA_DIR}/*') if os.path.isdir(d)])
    
    results_data = []
    object_counters = {}

    for i, obj_dir in enumerate(source_dirs[:NUM_OBJECTS_TO_PROCESS]):
        print(f"\n--- [{i+1}/{NUM_OBJECTS_TO_PROCESS}] Processing Object: {os.path.basename(obj_dir)} ---")
        
        try:
            source_obj_path = os.path.join(obj_dir, 'normalized_model.obj')
            source_img_path = os.path.join(obj_dir, 'image.jpg')
            if not os.path.exists(source_obj_path) or not os.path.exists(source_img_path):
                print(f"Skipping {obj_dir}, required files not found.")
                continue

            # 1. ì›ë³¸ ìº¡ì…˜ ìƒì„± ë° ê°ì²´ íƒ€ì… ì¶”ì¶œ
            original_text = generate_caption(models, source_img_path)
            object_type = extract_object_type(original_text)
            
            # 2. í…Œë§ˆ ì •ì˜
            themes = {
                "yellow": f"a yellow {object_type}",
                "cyberpunk": f"a cyberpunk {object_type}, made of dark chrome and glowing purple circuits"
            }

            for theme_name, edited_prompt in themes.items():
                # 3. ê²°ê³¼ ì €ì¥ í´ë” ìƒì„± (sofa_1, sofa_2, ...)
                object_counters[object_type] = object_counters.get(object_type, 0) + 1
                current_obj_name = f"{object_type}_{object_counters[object_type]}"
                result_dir = os.path.join(OUTPUT_DIR, current_obj_name)
                os.makedirs(result_dir, exist_ok=True)
                print(f"\n-- Processing theme '{theme_name}' for {current_obj_name} --")

                # 4. 3D í¸ì§‘ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                depth_map_path = create_depth_map(source_obj_path, os.path.join(result_dir, 'control_depth_map.png'))
                ref_image_path = create_ref_image(models, depth_map_path, edited_prompt, os.path.join(result_dir, 'ref_texture_image.png'))
                edited_mesh_path = texture_mesh(models, source_obj_path, ref_image_path, os.path.join(result_dir, 'edited_mesh.glb'))
                
                # 5. í‰ê°€ ì‹¤í–‰
                print(f"Evaluating similarity for {current_obj_name}...")
                score = evaluate_clip_similarity(models, source_obj_path, edited_mesh_path, original_text, edited_prompt)
                
                # 6. ê²°ê³¼ ì €ì¥
                shutil.copy(source_obj_path, os.path.join(result_dir, 'source_mesh.obj'))
                shutil.copy(source_img_path, os.path.join(result_dir, 'source_image.jpg'))
                with open(os.path.join(result_dir, 'source_caption.txt'), 'w') as f:
                    f.write(original_text)
                with open(os.path.join(result_dir, 'editing_prompt.txt'), 'w') as f:
                    f.write(edited_prompt)
                
                results_data.append({
                    'object_type': current_obj_name,
                    'editing_prompt': edited_prompt,
                    'clip_ds_score': f"{score:.4f}"
                })
                print(f"âœ… Finished processing {current_obj_name}. Score: {score:.4f}")

        except Exception as e:
            print(f"âŒ FAILED to process {obj_dir}. Error: {e}")
            import traceback
            traceback.print_exc()

    # 7. ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    csv_path = os.path.join(OUTPUT_DIR, 'results.csv')
    print(f"\nSaving final results to {csv_path}")
    with open(csv_path, 'w', newline='') as csvfile:
        fieldnames = ['object_type', 'editing_prompt', 'clip_ds_score']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results_data)

    print("\nğŸ‰ Batch processing complete! ğŸ‰")


if __name__ == '__main__':
    # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GPUì™€ ìƒë‹¹í•œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # xvfb-runì„ ì‚¬ìš©í•˜ì—¬ headless í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆ: xvfb-run --auto-servernum python run_batch_processing.py
    main()