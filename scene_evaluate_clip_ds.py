import os
import argparse
import torch
import numpy as np
import trimesh
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP ëª¨ë¸ ë¡œë“œ
clip_model = CLIPModel.from_pretrained(
    "openai/clip-vit-large-patch14",
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto"
)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")


def get_text_embedding_for_clip(text: str):
    """CLIP í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    inputs = clip_processor(
        text=[text],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=77
    ).to(DEVICE)
    feats = clip_model.get_text_features(**inputs)
    feats = torch.nn.functional.normalize(feats, dim=-1)
    return feats


def get_image_embedding_for_clip(pil_image: Image.Image):
    """CLIP ì´ë¯¸ì§€ ì„ë² ë”©"""
    inputs = clip_processor(images=pil_image, return_tensors="pt").to(DEVICE)
    feats = clip_model.get_image_features(**inputs)
    feats = torch.nn.functional.normalize(feats, dim=-1)
    return feats


def render_front_view(mesh_path, resolution=(224, 224)):
    """GLB/OBJ íŒŒì¼ì„ ì •ë©´ì—ì„œ ë Œë”ë§"""
    mesh = trimesh.load(mesh_path, force="scene")
    # ì¹´ë©”ë¼ ìœ„ì¹˜ë¥¼ Zì¶• ì•ìª½ìœ¼ë¡œ ë°°ì¹˜
    scene = mesh if isinstance(mesh, trimesh.Scene) else trimesh.Scene(mesh)
    camera_transform = np.array([
        [1, 0, 0, 0],
        [0, 1, 0, -2.0],   # yì¶• ë’¤ë¡œ ì¡°ê¸ˆ ì´ë™
        [0, 0, 1, 3.0],    # zì¶• ìœ„ë¡œ ì˜¬ë¦¼
        [0, 0, 0, 1]
    ])
    scene.camera_transform = camera_transform
    data = scene.save_image(resolution=resolution)
    return Image.open(trimesh.util.wrap_as_stream(data)).convert("RGB")


def compute_clip_directional_similarity(img1, img2, caption1, caption2):
    """ë‘ ì´ë¯¸ì§€ì™€ ë‘ í…ìŠ¤íŠ¸ ìº¡ì…˜ìœ¼ë¡œ CLIP DS ì ìˆ˜ ê³„ì‚°"""
    with torch.no_grad():
        t1 = get_text_embedding_for_clip(caption1)
        t2 = get_text_embedding_for_clip(caption2)
        x1 = get_image_embedding_for_clip(img1)
        x2 = get_image_embedding_for_clip(img2)

        delta_text = t1 - t2
        delta_image = x1 - x2

        eps = 1e-8
        if torch.linalg.norm(delta_text) < eps or torch.linalg.norm(delta_image) < eps:
            return 0.0

        sim = torch.nn.functional.cosine_similarity(delta_image, delta_text).mean().item()
    return float(sim)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Compare two scene styles with CLIP DS")
    parser.add_argument("--style1", type=str, required=True, help="ì²« ë²ˆì§¸ ìŠ¤íƒ€ì¼ ì´ë¦„ (ì˜ˆ: art_deco)")
    parser.add_argument("--style2", type=str, required=True, help="ë‘ ë²ˆì§¸ ìŠ¤íƒ€ì¼ ì´ë¦„ (ì˜ˆ: steampunk)")
    parser.add_argument(
        "--scene_dir",
        type=str,
        default="/source/sola/Kocca_3Dedit/scene_data/scene1",
        help="ì”¬ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” í´ë”"
    )
    args = parser.parse_args()

    # íŒŒì¼ ê²½ë¡œ
    scene1_path = os.path.join(args.scene_dir, f"edited_scene_{args.style1}_loc.glb")
    scene2_path = os.path.join(args.scene_dir, f"edited_scene_{args.style2}_loc.glb")

    if not os.path.exists(scene1_path) or not os.path.exists(scene2_path):
        raise FileNotFoundError(f"GLB íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:\n{scene1_path}\n{scene2_path}")

    print(f"ğŸ¨ Rendering scenes: {args.style1} vs {args.style2}")

    # ë Œë”ë§
    img1 = render_front_view(scene1_path)
    img2 = render_front_view(scene2_path)

    # ìº¡ì…˜
    caption1 = f"{args.style1} style living room"
    caption2 = f"{args.style2} style living room"

    # ì ìˆ˜ ê³„ì‚°
    score = compute_clip_directional_similarity(img1, img2, caption1, caption2)
    print(f"âœ… CLIP DS Score ({args.style1} â†” {args.style2}): {score:.4f}")
